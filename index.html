<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- NOTE: Do not scrape the code from this page directly, as it includes analytics tags. You are welcome to use the HTML in this page, but please do so by cloning the github repo linked at the bottom. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QSFRPR5L7E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QSFRPR5L7E');
  </script>

  <title>Wenyuan Zhang | Âº†ÊñáÊ∫ê</title>
  
  <meta name="author" content="Wenyuan Zhang | Âº†ÊñáÊ∫ê">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêÖ</text></svg>">
  <script src="script/functions.js"></script>
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:62%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenyuan Zhang | Âº†ÊñáÊ∫ê</name>
              </p>
              <p>
                I am currently a fourth-year Ph.D student in <a href="https://www.thss.tsinghua.edu.cn/">School of Software</a>, <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, advised by Prof. <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>.
                I got my bachelor's degree from <a href="https://www.thss.tsinghua.edu.cn/">School of Software</a>, <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a> in 2021.
              </p>
              <p>
                My research interests lie in the area of computer vision and graphics, including multi-view neural rendering, 3D reconstruction and video generation. 
              </p>
              <p style="text-align:center">
                <a href="mailto:zhangwen21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=qzH0hNAAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/wen-yuan-zhang/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="media/profile.jpg"><img style="width:90%;max-width:100%" alt="profile photo" src="media/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li><strong>04/2024:</strong> The extension of <a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a> on implicit representations is accepted to <b>TPAMI 2024</b>.
                  <li><strong>03/2024:</strong> Our paper <a href="https://weiqi-zhang.github.io/UDiFF/">UDiFF</a> on UDF-based 3D generative models is accepted to <b>CVPR 2024</b>.
                  <li><strong>03/2024:</strong> I will co-organize workshop <a href="https://emai-workshop.github.io/">EMbodied AI: Trends, Challenges, and Opportunities</a> in <b>ICIP 2024</b>.

                  <li><strong>02/2024:</strong> Our paper <a href="https://github.io">3D-OAE</a> on 3D foundation models is accepted to <b>ICRA 2024</b> for <strong><span style="color:#ff0000;">Oral</span></strong> presentation.

                  <li><strong>01/2024:</strong> Our work <a href="https://github.com/baaivision/Uni3D">Uni3D</a> on scaling up 3D foundation models is accepted to <b>ICLR 2024</b> <strong><span style="color:#ff0000;">(Spotlight)</span></strong>.
                  <li><strong>12/2023:</strong> Two papers on multi-view reconstruction and point upsampling is accepted to <b>AAAI 2024</b>.
                  <li><strong>10/2023:</strong> Releasing <a href="https://github.com/baaivision/Uni3D">Uni3D</a>, a unified 3D foundation model with <strong><span style="color:#ff0000;">one billion</span></strong> parameters.
                  </li>
                  <li><strong>09/2023:</strong> Our paper <a href="">VP2P-Match</a> on image to LiDAR point cloud registration is accepted to <b>NeurIPS 2023</b> <strong><span style="color:#ff0000;">(Spotlight)</span></strong>.
                  </li>
                  <li><strong>08/2023:</strong> Our paper <a href="https://github.com/junshengzhou/LevelSetUDF/">LevelSetUDF</a> on neural implicit representations is accepted to <b>ICCV 2023</b>.
                  </li>
                  <li><strong>06/2023:</strong> One paper on surface reconstruction that I advised is accepted to <b>SMI 2023</b> and <b>C&G 2023</b>.
                  </li>
                  <li><strong>03/2023:</strong> Our paper <a href="https://github.com/mabaorui/TowardsBetterGradient/">LSA-SDF</a> on implicit representation is accepted to <b>CVPR 2023</b>.
                  </li>
                  <li><strong>02/2023:</strong> Invited to give a talk about implicit surface reconstruction (<a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a>) at <b>AI TIME</b> (<a href="https://www.bilibili.com/video/BV1vX4y197G6/?spm_id_from=333.999.0.0&vd_source=12a96146b06f9a1687080af1ce34dcf1">Video</a>).
                  </li>

                  <li><strong>11/2022:</strong> Our paper <a href="https://github.com/lisj575/NeAF/">NeAF</a> on implicit point normal estimation is accepted to <b>AAAI 2023</b> <strong><span style="color:#ff0000;">(Oral)</span></strong>.
                  </li>

                  <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                  <div id="old_news" style="display: none;">

                  <li><strong>09/2022:</strong> Our paper <a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a> on surface reconstruction is accepted to <b>NeurIPS 2022</b>.
                  </li>
                  <li><strong>03/2022:</strong> Our paper <a href='https://github.com/junshengzhou/3DAttriFlow'>3DAttriFlow</a> on point cloud generation is accepted to <b>CVPR 2022.</b>
                  </li>
                  <li><strong>11/2021:</strong> My team won the <strong><span style="color:#ff0000;">3rd Place</span></strong> in the <a href='https://mvp-dataset.github.io/'>MVP Completion Challenge</a> (<b>ICCV 2021</b> Workshop).
                  </li>
                  <li><strong>10/2021:</strong> Our paper MSM-Vis on data visualization won the <strong><span style="color:#ff0000;">Best Poster Paper</span></strong> on <b>ChinaVR 2021</b>.
                  </li>
                  <li><strong>09/2021:</strong> Started master journey at <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>.
                  </li>

                  </div>

                </ul>
    
              </td>
            </tr>
            </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>(<strong>*</strong> Equal Contribution)</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="KlingAvatar_stop()" onmouseover="KlingAvatar_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="KlingAvatar" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/KlingAvatar_arxiv25_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/KlingAvatar_arxiv25_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function KlingAvatar_start() {
                  document.getElementById('KlingAvatar').style.opacity = "1";
                }
                function KlingAvatar_stop() {
                  document.getElementById('KlingAvatar').style.opacity = "0";
                }
                KlingAvatar_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://klingavatar.github.io/">
                <papertitle>Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis
                </papertitle>
              </a>
              <br>
              <a href="https://yikang98.github.io/">Yikang Ding*</a>, <a>Jiwen Liu</a>*, <strong>Wenyuan Zhang</strong>, <a href="https://zenmoore.github.io/">Zekun Wang</a>, <a href="https://vincenthu19.github.io/">Wentao Hu</a>, <a href="https://scholar.google.com/citations?user=WhwTrhcAAAAJ&hl=zh-CN">Liyuan Cui</a>, <a>Mingming Lao</a>, <a>Yingchao Shao</a>, <a>Hui Liu</a>, <a>Xiaohan Li</a>, <a href="https://scholar.google.com/citations?user=eH4ofxMAAAAJ&hl=zh-CN">Ming Chen</a>, <a href="https://openreview.net/profile?id=~Xiaoqiang_Liu3">Xiaoqiang Liu</a>, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://magicwpf.github.io/">Pengfei Wan</a>
              <br>
              <em>Technical Report. arXiv:2509.09595</em>, 2025
              <br>
              <a href="https://klingavatar.github.io/">project page</a> | 
              <a href="https://arxiv.org/abs/2509.09595">arXiv</a>
        
              <p align="justify", style="font-size:13px">
                we introduce Kling-Avatar, a novel framework that unifies multimodal instruction understanding with cascaded long-duration video generation for avatar animation synthesis.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="MIDAS_stop()" onmouseover="MIDAS_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="MIDAS" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/MIDAS_arxiv25_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/MIDAS_arxiv25_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function MIDAS_start() {
                  document.getElementById('MIDAS').style.opacity = "1";
                }
                function MIDAS_stop() {
                  document.getElementById('MIDAS').style.opacity = "0";
                }
                MIDAS_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://chenmingthu.github.io/milm">
                <papertitle>MIDAS: Multimodal Interactive Digital-humAn Synthesis via Real-time Autoregressive Video Generation
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=eH4ofxMAAAAJ&hl=zh-CN">Ming Chen*</a>, <a href="https://scholar.google.com/citations?user=WhwTrhcAAAAJ&hl=zh-CN">Liyuan Cui*</a>, <strong>Wenyuan Zhang*</strong>, <a href="https://scholar.google.com.hk/citations?user=r_aOHlQAAAAJ&hl">Haoxian Zhang</a>, <a href="https://ieeexplore.ieee.org/author/37085821566">Yan Zhou</a>, <a href="">Xiaohan Li</a>, <a href="https://github.com/FreemanTom">Songlin Tang</a>, <a href="">Jiwen Liu</a>, <a href="">Borui Liao</a>, <a href="https://harryxd2018.github.io/">Hejia Chen</a>,  <a href="https://openreview.net/profile?id=~Xiaoqiang_Liu3">Xiaoqiang Liu</a>, <a href="https://magicwpf.github.io/">Pengfei Wan</a>
              <br>
              <em>Technical Report. arXiv:2508.19320</em>, 2025
              <br>
              <a href="https://chenmingthu.github.io/milm">project page</a> | 
              <a href="https://www.arxiv.org/abs/2508.19320">arXiv</a>
        
              <p align="justify", style="font-size:13px">
                We introduce an autoregressive human video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="GAP_stop()" onmouseover="GAP_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="GAP" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/GAP_iccv25_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/GAP_iccv25.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function GAP_start() {
                  document.getElementById('GAP').style.opacity = "1";
                }
                function GAP_stop() {
                  document.getElementById('GAP').style.opacity = "0";
                }
                GAP_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://weiqi-zhang.github.io/GAP/">
                <papertitle>GAP: Gaussianize Any Point Clouds with Text Guidance
                </papertitle>
              </a>
              <br>
              <a href="https://weiqi-zhang.github.io/">Weiqi Zhang*</a>, <a href="https://junshengzhou.github.io/">Junsheng Zhou*</a>, <a href="https://github.com/mts246">Haotian Geng*</a>, <strong>Wenyuan Zhang</strong>, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision <b>(ICCV)</b></em>, 2025
              <br>
              <a href="https://weiqi-zhang.github.io/GAP/">project page</a> | 
              <a href="https://www.arxiv.org/abs/2508.05631">arXiv</a> | 
              <a href="https://github.com/weiqi-zhang/GAP">code</a>
        
              <p align="justify", style="font-size:13px">
                We present GAP, which gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance via depth-aware diffusion and surface-anchored optimization.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="NeRFPrior_stop()" onmouseover="NeRFPrior_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="NeRFPrior" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/NeRFPrior_cvpr25_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/NeRFPrior_cvpr25_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function NeRFPrior_start() {
                  document.getElementById('NeRFPrior').style.opacity = "1";
                }
                function NeRFPrior_stop() {
                  document.getElementById('NeRFPrior').style.opacity = "0";
                }
                NeRFPrior_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://wen-yuan-zhang.github.io/NeRFPrior/">
                <papertitle>NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction
                </papertitle>
              </a>
              <br>
              <strong>Wenyuan Zhang</strong>, <a href="https://emily-jia.github.io/personal-web//">Emily Yue-ting Jia</a>, <a href="https://junshengzhou.github.io/">Junsheng Zhou</a>, <a href="https://mabaorui.github.io/">Baorui Ma</a>, <a href="https://openreview.net/profile?id=%7EKanle_Shi2">Kanle Shi</a>, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2025 (<strong><span style="color:#ff0000;">Highlight</span></strong>)
              <br>
              <a href="https://wen-yuan-zhang.github.io/NeRFPrior/">project page</a> |        
              <a href="https://arxiv.org/abs/2503.18361">arXiv</a>
              <!-- <a href="https://github.com/yulunwu0108/Sparis">code</a> -->
        
              <p align="justify", style="font-size:13px">
                We present NeRFPrior, which adopts a neural radiance field as a prior to learn signed distance fields using volume rendering for indoor scene surface reconstruction. 
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="MonoInstance_stop()" onmouseover="MonoInstance_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="MonoInstance" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/MonoInstance_cvpr25_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/MonoInstance_cvpr25_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function MonoInstance_start() {
                  document.getElementById('MonoInstance').style.opacity = "1";
                }
                function MonoInstance_stop() {
                  document.getElementById('MonoInstance').style.opacity = "0";
                }
                MonoInstance_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://wen-yuan-zhang.github.io/MonoInstance/">
                <papertitle>MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction
                </papertitle>
              </a>
              <br>
              <strong>Wenyuan Zhang</strong>, Yixiao Yang, <a href="https://github.com/alvin528">Han Huang</a>, <a href="https://github.com/hanl2010">Liang Han</a>, <a href="https://openreview.net/profile?id=%7EKanle_Shi2">Kanle Shi</a>, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2025
              <br>
              <a href="https://wen-yuan-zhang.github.io/MonoInstance/">project page</a> |            
              <a href="https://arxiv.org/abs/2503.18363">arXiv</a>
              <!-- <a href="https://github.com/yulunwu0108/Sparis">code</a> -->
        
              <p align="justify", style="font-size:13px">
                we propose a general approach that explores the uncertainty of monocular depths to provide enhanced geometric priors for neural rendering and reconstruction. 
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="Sparis_stop()" onmouseover="Sparis_start()">
            <td width="28%">
              <div class="one">
                <!-- <div class="two" id="Sparis" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/VRP_eccv24_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div> -->
                <img src="media/Sparis_aaai25.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function Sparis_start() {
                  document.getElementById('Sparis').style.opacity = "1";
                }
                function Sparis_stop() {
                  document.getElementById('Sparis').style.opacity = "0";
                }
                Sparis_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://yulunwu0108.github.io/Sparis/">
                <papertitle>Sparis: Neural Implicit Surface Reconstruction of Indoor Scenes from Sparse Views
                </papertitle>
              </a>
              <br>
              <a href="https://yulunwu0108.github.io/">Yulun Wu*</a>, <a href="https://github.com/alvin528">Han Huang*</a>, <strong>Wenyuan Zhang</strong>, Chao Deng, Ge Gao, Ming Gu, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>
              <br>
              <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2025 (<strong><span style="color:#ff0000;">Oral</span></strong>)
              <br>
              <a href="https://yulunwu0108.github.io/Sparis/">project page</a> |             
              <a href="https://arxiv.org/abs/2501.01196">arXiv</a> |
              <a href="https://github.com/yulunwu0108/Sparis">code</a>
        
              <p align="justify", style="font-size:13px">
                We propose a method that introduces a novel prior based on inter-image matching information for indoor surface reconstruction from sparse views.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="GSPull_stop()" onmouseover="GSPull_start()">
            <td width="28%">
              <div class="one">
                <!-- <div class="two" id="GSPull" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/VRP_eccv24_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div> -->
                <img src="media/GSPull_nips24.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function GSPull_start() {
                  document.getElementById('GSPull').style.opacity = "1";
                }
                function GSPull_stop() {
                  document.getElementById('GSPull').style.opacity = "0";
                }
                GSPull_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://wen-yuan-zhang.github.io/GS-Pull/">
                <papertitle>Neural Signed Distance Function Inference through Splatting 3D Gaussians Pulled on Zero-Level Set
                </papertitle>
              </a>
              <br>
              <strong>Wenyuan Zhang</strong>,<a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>Advances in Neural Information Processing Systems <b>(NeurIPS)</b></em>, 2024
              <br>
              <a href="https://wen-yuan-zhang.github.io/GS-Pull/">project page</a> |             
              <a href="https://arxiv.org/abs/2410.14189">arXiv</a>  |
              <a href="https://github.com/wen-yuan-zhang/GS-Pull">code</a>
        
              <p align="justify", style="font-size:13px">
                We propose a method that seamlessly merge 3DGS with the learning of neural SDFs. To this end, we dynamically align 3D Gaussians on the zero-level set of the neural SDF. Meanwhile, we update the neural SDF by pulling neighboring space to the pulled 3D Gaussians, which progressively refine the signed distance field near the surface.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="VRPNeRF_stop()" onmouseover="VRPNeRF_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="VRPNeRF" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/VRP_eccv24_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/VRP_eccv24_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function VRPNeRF_start() {
                  document.getElementById('VRPNeRF').style.opacity = "1";
                }
                function VRPNeRF_stop() {
                  document.getElementById('VRPNeRF').style.opacity = "0";
                }
                VRPNeRF_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://wen-yuan-zhang.github.io/VolumeRenderingPriors/">
                <papertitle>Learning Unsigned Distance Functions from Multi-view Images with Volume Rendering Priors
                </papertitle>
              </a>
              <br>
              <strong>Wenyuan Zhang</strong>, <a href="https://openreview.net/profile?id=%7EKanle_Shi2">Kanle Shi</a>, <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>European Conference on Computer Vision <b>(ECCV)</b></em>, 2024
              <br>
              <a href="https://wen-yuan-zhang.github.io/VolumeRenderingPriors/">project page</a> |             
              <a href="https://arxiv.org/abs/2407.16396">arXiv</a> |
              <a href="https://github.com/wen-yuan-zhang/VolumeRenderingPriors">code</a>
        
              <p align="justify", style="font-size:13px">
                We present a novel differentiable renderer to infer UDFs from multi-view images. Instead of using hand-crafted equations, our differentiable renderer is a neural network which is pre-trained in a data-driven manner, dubbed volume rendering priors.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="VP2P_stop()" onmouseover="VP2P_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="VP2P" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/VP2PMatching_nips23_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/VP2PMatching_nips23_before.png" width="225">
      
              </div>

              <script type="text/javascript">
                function VP2P_start() {
                  document.getElementById('VP2P').style.opacity = "1";
                }
                function VP2P_stop() {
                  document.getElementById('VP2P').style.opacity = "0";
                }
                VP2P_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://github.com/junshengzhou/VP2P-Match">
                <papertitle>Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching
                </papertitle>
              </a>
              <br>
              <a href="https://junshengzhou.github.io/">Junsheng Zhou*</a>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, <strong>Wenyuan Zhang</strong>, <a href="http://mmvc.engineering.nyu.edu/">Yi Fang</a>,
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>Conference on Neural Information Processing Systems <b>(NeurIPS)</b></em>, 2023  (<strong><span style="color:#ff0000;">Spotlight</span></strong>)
              <br>             
              <a href="https://arxiv.org/abs/2312.04060">arXiv</a> |
              <a href="https://github.com/junshengzhou/VP2P-Match">code</a>
      
              <p align="justify", style="font-size:13px">We design a triplet network to learn VoxelPoint-to-Pixel matching via a differentiable probabilistic PnP solver.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="ICCVUDF_stop()" onmouseover="ICCVUDF_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="ICCVUDF" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/FastLearning_tip23_after.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/FastLearning_tip23_before.png" width="225">

              </div>
              <script type="text/javascript">
                function ICCVUDF_start() {
                  document.getElementById('ICCVUDF').style.opacity = "1";
                }
                function ICCVUDF_stop() {
                  document.getElementById('ICCVUDF').style.opacity = "0";
                }
                ICCVUDF_stop()
              </script>
            </td>

            <td width="65%" valign="top">
              <a href="https://wen-yuan-zhang.github.io/Fast-Learning-NeRF/">
                <papertitle>Fast Learning Radiance Fields by Shooting Much Fewer Rays
                </papertitle>
              </a>
              <br>
              <strong>Wenyuan Zhang</strong>, Ruofan Xing</a>, <a href="https://yunfan.zone/">Yunfan Zeng</a>,
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://openreview.net/profile?id=%7EKanle_Shi2">Kanle Shi</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE Transactions on Image Processing <b>(TIP)</b></em>, 2023
              <br>
              <a href="https://wen-yuan-zhang.github.io/Fast-Learning-NeRF/">project page</a> |             
              <a href="https://arxiv.org/abs/2208.06821">arXiv</a> |
              <a href="https://github.com/wen-yuan-zhang/Fast-Learning-NeRF">code</a>

              <p align="justify", style="font-size:13px">
                We introduce a general strategy to speed up the learning procedure for almost all radiance fields based methods. The key idea is to reduce the redundancy by shooting much fewer rays in the multi-view volume rendering procedure.
              </p>
            </td>
          </tr>
          </tbody>
        </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <ul>

                  <li> <strong>Comprehensive Excellent Scholarship</strong> (ÁªºÂêà‰ºòÁßÄÂ•ñÂ≠¶Èáë) at Tsinghua University, 2024,2023,2021,2019.
                  </li>
                  <li> <strong>Excellent undergraduate thesis</strong> (Êú¨Áßë‰ºòÁßÄÊØï‰∏öËÆ∫Êñá) at Tsinghua University, 2021.
                  </li>
                  <li> <strong>Excellent graduates</strong> (Êú¨Áßë‰ºòËâØÊØï‰∏öÁîü) at Tsinghua University, 2021.
                  </li>
                </ul>
    
              </td>
            </tr>
            </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Academic Services</heading>
                <ul>
                  <li> 
                    <strong>Program Committee Member</strong>: AAAI-26
                  </li>
                  <li> 
                    <strong>Conference Reviewer</strong>: ACMMM-25, NeurIPS-24/25, ICCV-25, ICML-25, CVPR-25, ICLR-25, WACV-25/26, IJCAI-24
                  </li>
                  <li> 
                    <strong>Journal Reviewer</strong>: IEEE TIP, IEEE TVCG, CAD
                  </li>
                  

                </ul>
    
              </td>
            </tr>
            </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>More About Me</heading>
                  <ul>
                    <li> 
                      I currently serve as the T.A. of the undergraduate course "Fundamentals of Programming" (Á®ãÂ∫èËÆæËÆ°Âü∫Á°Ä), which is taught by Prof. <a href="https://yushen-liu.github.io">Yu-Shen Liu</a>. I am also serving as an undergraduate advisor (Êú¨ÁßëÁîüËæÖÂØºÂëò) at School of Software. 
                      My hobbies include playing the flute, running, and singing. Feel free to contact me!
                    </li>
                    
  
                  </ul>
      
                </td>
              </tr>
              </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
              <br>
              <p align="right">
                <font size="2">
                template adapted from <a href="https://junshengzhou.github.io/"><font size="2">this awesome website</font></a>
                <br>
                Last updated: September 2025
                <br>
                
              </font>
              </p>
              </td>
            </tr>
          </table>
      </td>
    </tr>
    

</html>
